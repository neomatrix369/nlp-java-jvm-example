{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "* [Find out the version info of the underlying JDK/JVM on which this notebook is running](#Find-out-the-version-info-of-the-underlying-JDK/JVM-on-which-this-notebook-is-running)\n",
    "* [Import the Apache OpenNLP jar files located in the lib folder of the NLP library](#Import-the-Apache-OpenNLP-jar-files-located-in-the-lib-folder-of-the-NLP-library)\n",
    "* Java bindings (Java API)\n",
    " * [Language Detector API](#Language-Detector-API)\n",
    " * [Sentence Detection API](#Sentence-Detection-API)\n",
    " * [Tokenizer API](#Tokenizer-API)\n",
    " * [Name Finder API](#Name-Finder-API)\n",
    " * [More Name Finder API examples](#More-Name-Finder-API-examples)\n",
    " * [Parts of speech (POS) Tagger API](#Parts-of-speech-(POS)-Tagger-API)\n",
    " * [Chunking API](#Chunking-API)\n",
    " * [Parsing API](#Parsing-API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out the version info of the underlying JDK/JVM on which this notebook is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.version: 11.0.4\n"
     ]
    }
   ],
   "source": [
    "System.out.println(\"java.version: \" + System.getProperty(\"java.version\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.specification.version: 11\n",
      "java.runtime.version: 11.0.4+11\n"
     ]
    }
   ],
   "source": [
    "System.out.println(\"java.specification.version: \" + System.getProperty(\"java.specification.version\"));\n",
    "System.out.println(\"java.runtime.version: \" + System.getProperty(\"java.runtime.version\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java runtime VM version: 11.0.4+11\n"
     ]
    }
   ],
   "source": [
    "import java.lang.management.ManagementFactory;\n",
    "\n",
    "System.out.println(\"java runtime VM version: \" + ManagementFactory.getRuntimeMXBean().getVmVersion());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Apache OpenNLP jar files located in the lib folder of the NLP library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache OpenNLP library: find the folder containing the jar files**\n",
    "We know during the building of this container that the library would be exploded in the ../shared folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\n",
      "docs\n",
      "issuesFixed\n",
      "lang\n",
      "lib\n",
      "LICENSE\n",
      "NOTICE\n",
      "README.html\n"
     ]
    }
   ],
   "source": [
    "%system ls ../shared/apache-opennlp-1.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "List<String> addedJars = %jars \"../shared/apache-opennlp-1.9.1/lib/*.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// Uncomment below line to see list of jars imported\n",
    "// addedJars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to find out about each of the classes enlisted in this notebook please refer to the Java API JavaDocs at  https://opennlp.apache.org/docs/1.9.1/apidocs/opennlp-tools/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of models used in this notebook can be found at https://opennlp.apache.org/models.html and http://opennlp.sourceforge.net/models-1.5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Detector API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the Language detection model and place it in the ../shared folder (using curl or wget and the system cell magic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if model langdetect-183.bin (en) exists...\n",
      "Downloading model langdetect-183.bin (en) in '../shared/'\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 18 10.0M   18 1878k    0     0  2310k      0  0:00:04 --:--:--  0:00:04 2310k\n",
      " 76 10.0M   76 7859k    0     0  4339k      0  0:00:02  0:00:01  0:00:01 4337k\n",
      "100 10.0M  100 10.0M    0     0  4914k      0  0:00:02  0:00:02 --:--:-- 4914k\n"
     ]
    }
   ],
   "source": [
    "%system ../opennlp/detectLanguage.sh --downloadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the Language detecting model called langdetect-183.bin from the \"../shared/\" folder, and show a simple example detecting a language of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Started...]\n",
      "Sentence: This is a sample text.\n",
      "Best language: lat\n",
      "Best language confidence: 0.017774467481479657\n",
      "\n",
      "Predict languages (with confidence): [tur (0.009708737864077673), bel (0.009708737864077673), san (0.009708737864077673), ara (0.009708737864077673), mon (0.009708737864077673), tel (0.009708737864077673), sin (0.009708737864077673), pes (0.009708737864077673), min (0.009708737864077673), cmn (0.009708737864077673), aze (0.009708737864077673), fao (0.009708737864077673), ita (0.009708737864077673), ceb (0.009708737864077673), mkd (0.009708737864077673), eng (0.009708737864077673), nno (0.009708737864077673), lvs (0.009708737864077673), kor (0.009708737864077673), som (0.009708737864077673), swa (0.009708737864077673), hun (0.009708737864077673), fra (0.009708737864077673), nld (0.009708737864077673), mlt (0.009708737864077673), bak (0.009708737864077673), ekk (0.009708737864077673), ron (0.009708737864077673), gle (0.009708737864077673), hin (0.009708737864077673), est (0.009708737864077673), tha (0.009708737864077673), slk (0.009708737864077673), ltz (0.009708737864077673), kan (0.009708737864077673), eus (0.009708737864077673), epo (0.009708737864077673), bos (0.009708737864077673), pol (0.009708737864077673), nep (0.009708737864077673), lit (0.009708737864077673), war (0.009708737864077673), srp (0.009708737864077673), ces (0.009708737864077673), che (0.009708737864077673), lav (0.009708737864077673), nds (0.009708737864077673), dan (0.009708737864077673), mar (0.009708737864077673), nan (0.009708737864077673), glg (0.009708737864077673), gsw (0.009708737864077673), fry (0.009708737864077673), uzb (0.009708737864077673), mal (0.009708737864077673), vol (0.009708737864077673), fas (0.009708737864077673), msa (0.009708737864077673), cym (0.009708737864077673), nob (0.009708737864077673), ben (0.009708737864077673), kaz (0.009708737864077673), heb (0.009708737864077673), bre (0.009708737864077673), jav (0.009708737864077673), sqi (0.009708737864077673), kir (0.009708737864077673), cat (0.009708737864077673), oci (0.009708737864077673), vie (0.009708737864077673), kat (0.009708737864077673), tam (0.009708737864077673), tgk (0.009708737864077673), mri (0.009708737864077673), slv (0.009708737864077673), lat (0.009708737864077673), tgl (0.009708737864077673), pan (0.009708737864077673), swe (0.009708737864077673), lim (0.009708737864077673), tat (0.009708737864077673), ell (0.009708737864077673), afr (0.009708737864077673), pus (0.009708737864077673), isl (0.009708737864077673), sun (0.009708737864077673), urd (0.009708737864077673), hye (0.009708737864077673), hrv (0.009708737864077673), ast (0.009708737864077673), rus (0.009708737864077673), spa (0.009708737864077673), ind (0.009708737864077673), pnb (0.009708737864077673), bul (0.009708737864077673), plt (0.009708737864077673), deu (0.009708737864077673), zul (0.009708737864077673), ukr (0.009708737864077673), jpn (0.009708737864077673), por (0.009708737864077673), guj (0.009708737864077673), fin (0.009708737864077673)]\n",
      "[...Finished]\n"
     ]
    }
   ],
   "source": [
    "import java.io.InputStream;\n",
    "import java.io.FileInputStream;\n",
    "import opennlp.tools.langdetect.LanguageDetectorModel;\n",
    "import opennlp.tools.langdetect.LanguageDetectorME;\n",
    "import opennlp.tools.langdetect.LanguageDetector;\n",
    "import opennlp.tools.langdetect.Language;\n",
    "import java.util.Arrays;\n",
    "\n",
    "System.out.println(\"[Started...]\");\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/langdetect-183.bin\")) {\n",
    "    LanguageDetectorModel langModel = new LanguageDetectorModel(modelIn);\n",
    "    String inputText = \"This is a sample text.\";\n",
    "    System.out.println(\"Sentence: \" + inputText);\n",
    "\n",
    "    // Get the most probable language\n",
    "    LanguageDetector myCategorizer = new LanguageDetectorME(langModel);\n",
    "    Language bestLanguage = myCategorizer.predictLanguage(inputText);\n",
    "    System.out.println(\"Best language: \" + bestLanguage.getLang());\n",
    "    System.out.println(\"Best language confidence: \" + bestLanguage.getConfidence());\n",
    "\n",
    "    // Get an array with the most probable languages\n",
    "    Language[] languages = myCategorizer.predictLanguages(\"\");\n",
    "    System.out.println(\"\");\n",
    "    System.out.println(\"Predict languages (with confidence): \" + Arrays.toString(languages));\n",
    "}\n",
    "System.out.println(\"[...Finished]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apparantly it detects this to be Latin, instead of English \n",
    "maybe the language detecting model needs more training.\n",
    "See https://opennlp.apache.org/docs/1.9.1/manual/opennlp.html#tools.langdetect.training on how this can be achieved**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Detection API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the Sentence detection model and place it in the ../shared folder (using curl or wget and the system cell magic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if model en-sent.bin (en) exists...\n",
      "Downloading model en-sent.bin (en) in '../shared/'\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 98533  100 98533    0     0   126k      0 --:--:-- --:--:-- --:--:--  126k\n"
     ]
    }
   ],
   "source": [
    "%system ../opennlp/detectSentence.sh --downloadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the [en] Sentence detecting model called en-sent.bin from the \"../shared/\" folder, and show a simple example detecting a language of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Started...]\n",
      "Sentence:   First sentence. Second sentence. \n",
      "[First sentence., Second sentence.]\n",
      "\n",
      "[[2..17), [18..34)]\n",
      "[...Finished]\n"
     ]
    }
   ],
   "source": [
    "import java.io.InputStream;\n",
    "import java.io.FileInputStream;\n",
    "import opennlp.tools.sentdetect.SentenceModel;\n",
    "import opennlp.tools.sentdetect.SentenceDetectorME;\n",
    "import opennlp.tools.util.Span;\n",
    "import java.util.Arrays;\n",
    "\n",
    "System.out.println(\"[Started...]\");\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/en-sent.bin\")) {\n",
    "  SentenceModel model = new SentenceModel(modelIn);\n",
    "  SentenceDetectorME sentenceDetector = new SentenceDetectorME(model);\n",
    "  String sentence = \"  First sentence. Second sentence. \";\n",
    "  System.out.println(\"Sentence: \" + sentence);\n",
    "  String sentences[] = sentenceDetector.sentDetect(sentence);\n",
    "  System.out.println(Arrays.toString(sentences));\n",
    "  Span sentencesUsingSpan[] = sentenceDetector.sentPosDetect(sentence);\n",
    "  System.out.println();\n",
    "  System.out.println(Arrays.toString(sentencesUsingSpan));\n",
    "}\n",
    "\n",
    "System.out.println(\"[...Finished]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see the two ways to use the SentenceDetect API to detect sentences in a piece of text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the Tokenizer model and place it in the ../shared folder (using curl or wget and the system cell magic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if model en-token.bin (en) exists...\n",
      "Downloading model en-token.bin (en) in '../shared/'\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 76  429k   76  327k    0     0   321k      0  0:00:01  0:00:01 --:--:--  321k\n",
      "100  429k  100  429k    0     0   361k      0  0:00:01  0:00:01 --:--:--  361k\n",
      "Checking if model en-token.bin (en) exists...\n",
      "Found model en-token.bin (en) in '../shared/'\n"
     ]
    }
   ],
   "source": [
    "%system ../opennlp/tokenizer.sh  --method learnable  --downloadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the [en] Tokenizer model called en-token.bin from the ../shared folder and show a simple example of tokenization of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Started...]\n",
      "Sentence: An input sample sentence.\n",
      "[An, input, sample, sentence, .]\n",
      "Probabilities of each of the tokens above\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9956236737394807\n",
      "1.0\n",
      "\n",
      "[[0..2), [3..8), [9..15), [16..24), [24..25)]\n",
      "[...Finished]\n"
     ]
    }
   ],
   "source": [
    "import java.io.InputStream;\n",
    "import java.io.FileInputStream;\n",
    "import opennlp.tools.tokenize.TokenizerModel;\n",
    "import opennlp.tools.tokenize.TokenizerME;\n",
    "import opennlp.tools.util.Span;\n",
    "import java.util.Arrays;\n",
    "\n",
    "System.out.println(\"[Started...]\");\n",
    "try(InputStream modelIn = new FileInputStream(\"../shared/en-token.bin\")) {\n",
    "    TokenizerModel model = new TokenizerModel(modelIn);\n",
    "    TokenizerME tokenizer = new TokenizerME(model);\n",
    "    String sentence = \"An input sample sentence.\";\n",
    "    System.out.println(\"Sentence: \" + sentence);    \n",
    "    String tokens[] = tokenizer.tokenize(sentence);\n",
    "    System.out.println(Arrays.toString(tokens));\n",
    "    double tokensProbabilies[] = tokenizer.getTokenProbabilities();\n",
    "    System.out.println(\"Probabilities of each of the tokens above\");\n",
    "    Arrays.stream(tokensProbabilies).forEach(System.out::println);\n",
    "    System.out.println();\n",
    "    Span tokensUsingSpans[] = tokenizer.tokenizePos(sentence);\n",
    "    System.out.println(Arrays.toString(tokensUsingSpans));\n",
    "}\n",
    "System.out.println(\"[...Finished]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Finder API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the Name Finder model and place it in the ../shared folder (using curl or wget and the system cell magic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if model en-ner-person.bin (en) exists...\n",
      "Downloading model en-ner-person.bin (en) in '../shared/'\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 5085k    0 34932    0     0  66410      0  0:01:18 --:--:--  0:01:18 66284\n",
      " 23 5085k   23 1220k    0     0   800k      0  0:00:06  0:00:01  0:00:05  799k\n",
      " 40 5085k   40 2058k    0     0   668k      0  0:00:07  0:00:03  0:00:04  668k\n",
      " 51 5085k   51 2610k    0     0   734k      0  0:00:06  0:00:03  0:00:03  734k\n",
      " 60 5085k   60 3078k    0     0   613k      0  0:00:08  0:00:05  0:00:03  613k\n",
      " 80 5085k   80 4099k    0     0   678k      0  0:00:07  0:00:06  0:00:01  736k\n",
      " 80 5085k   80 4099k    0     0   581k      0  0:00:08  0:00:07  0:00:01  520k\n",
      " 89 5085k   89 4568k    0     0   606k      0  0:00:08  0:00:07  0:00:01  563k\n",
      "100 5085k  100 5085k    0     0   667k      0  0:00:07  0:00:07 --:--:--  608k\n"
     ]
    }
   ],
   "source": [
    "%system ../opennlp/nameFinder.sh --method person --downloadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the [en]  Name finder model called en-ner-person.bin from the ../shared folder and and show a simple example detecting/finding a name of a person in two different sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Started...]\n",
      "Sentence: [Pierre, is, from, Paris, France.]\n",
      "[[0..1) person]\n",
      "Sentence: [John, is, from, London, England.]\n",
      "[[0..1) person]\n",
      "[...Finished]\n"
     ]
    }
   ],
   "source": [
    "import java.io.InputStream;\n",
    "import java.io.FileInputStream;\n",
    "import opennlp.tools.namefind.TokenNameFinderModel;\n",
    "import opennlp.tools.namefind.NameFinderME;\n",
    "import opennlp.tools.util.Span;\n",
    "import java.util.Arrays;\n",
    "\n",
    "System.out.println(\"[Started...]\");\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/en-ner-person.bin\")) {\n",
    "   TokenNameFinderModel model = new TokenNameFinderModel(modelIn);\n",
    "   NameFinderME nameFinder = new NameFinderME(model);\n",
    "   // The sentence has to be split into words and passed to the Name finder function\n",
    "   String documents[][][] = new String[][][] {{{\"Pierre\",\"is\", \"from\", \"Paris\", \"France.\"}, {\"John\", \"is\", \"from\", \"London\", \"England.\"}}};\n",
    "   for (String document[][]: documents) {\n",
    "      for (String sentence[]: document) {\n",
    "          System.out.println(\"Sentence: \" + Arrays.toString(sentence));\n",
    "          Span nameSpans[] = nameFinder.find(sentence);\n",
    "          System.out.println(Arrays.toString(nameSpans));\n",
    "      }\n",
    "      nameFinder.clearAdaptiveData();\n",
    "   }\n",
    "}\n",
    "System.out.println(\"[...Finished]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see above, it has detected the name of the person in both sentences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Name Finder API examples\n",
    "\n",
    "There are a handful more Name Finder related models i.e.\n",
    "\n",
    "- Name Finder Date\n",
    "- Name Finder Location\n",
    "- Name Finder Money\n",
    "- Name Finder Organization\n",
    "- Name Finder Percentage\n",
    "- Name Finder Time\n",
    "\n",
    "Their model names go by these names respectively:\n",
    "\n",
    "- en-ner-date.bin\n",
    "- en-ner-location.bin\n",
    "- en-ner-money.bin\n",
    "- en-ner-organization.bin\n",
    "- en-ner-percentage.bin\n",
    "- en-ner-time.bin\n",
    "\n",
    "and can be found at the same location all other models are found at, i.e. http://opennlp.sourceforge.net/models-1.5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of speech (POS) Tagger API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the PoS Tagger model and place it in the ../shared folder (using curl or wget and the system cell magic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if model en-pos-maxent.bin (en) exists...\n",
      "Downloading model en-pos-maxent.bin (en) in '../shared/'\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 5562k    0 44244    0     0  72176      0  0:01:18 --:--:--  0:01:18 72058\n",
      " 24 5562k   24 1349k    0     0   871k      0  0:00:06  0:00:01  0:00:05  871k\n",
      " 36 5562k   36 2013k    0     0   789k      0  0:00:07  0:00:02  0:00:05  789k\n",
      " 46 5562k   46 2563k    0     0   703k      0  0:00:07  0:00:03  0:00:04  703k\n",
      " 64 5562k   64 3587k    0     0   684k      0  0:00:08  0:00:05  0:00:03  684k\n",
      " 64 5562k   64 3588k    0     0   598k      0  0:00:09  0:00:06  0:00:03  658k\n",
      " 82 5562k   82 4611k    0     0   666k      0  0:00:08  0:00:06  0:00:02  607k\n",
      " 82 5562k   82 4611k    0     0   581k      0  0:00:09  0:00:07  0:00:02  483k\n",
      "100 5562k  100 5562k    0     0   663k      0  0:00:08  0:00:08 --:--:--  632k\n"
     ]
    }
   ],
   "source": [
    "%system ../opennlp/posTagger.sh --method maxent --downloadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the [en] PoS model called en-pos-maxent.bin from the ../shared folder, and show a simple example tagging parts of speech in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Started...]\n",
      "Sentence: [Most, large, cities, in, the, US, had, morning, and, afternoon, newspapers, .]\n",
      "[JJS, JJ, NNS, IN, DT, NNP, VBD, NN, CC, NN, NNS, .]\n",
      "\n",
      "Probabilities of tags: \n",
      "0.6005488809717314\n",
      "0.9346347227057236\n",
      "0.9928943439421191\n",
      "0.993711911129381\n",
      "0.9959619800700815\n",
      "0.9632635300742168\n",
      "0.96904256131942\n",
      "0.936549747737236\n",
      "0.9706281118634225\n",
      "0.8831901977922334\n",
      "0.9711019283924753\n",
      "0.9931572030890747\n",
      "\n",
      "Tags as sequences (contains probabilities: \n",
      "[-0.9196402685290461 [JJS, JJ, NNS, IN, DT, NNP, VBD, NN, CC, NN, NNS, .], -1.4538683571912276 [RBS, JJ, NNS, IN, DT, NNP, VBD, NN, CC, NN, NNS, .], -5.124416242584632 [JJS, JJ, NNS, IN, DT, PRP, VBD, NN, CC, NN, NNS, .]]\n",
      "[...Finished]\n"
     ]
    }
   ],
   "source": [
    "import java.io.InputStream;\n",
    "import java.io.FileInputStream;\n",
    "import opennlp.tools.postag.POSModel;\n",
    "import opennlp.tools.postag.POSTaggerME;\n",
    "import opennlp.tools.util.Sequence;\n",
    "import java.util.Arrays;\n",
    "\n",
    "System.out.println(\"[Started...]\");\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/en-pos-maxent.bin\")) {\n",
    "    POSModel model = new POSModel(modelIn);\n",
    "    POSTaggerME tagger = new POSTaggerME(model);\n",
    "\n",
    "    // The sentence has to be split into words and passed to the POS Tagger function\n",
    "    String sentence[] = new String[]{\"Most\", \"large\", \"cities\", \"in\", \"the\", \"US\", \"had\",\n",
    "                             \"morning\", \"and\", \"afternoon\", \"newspapers\", \".\"};\n",
    "    System.out.println(\"Sentence: \" + Arrays.toString(sentence));\n",
    "    String tags[] = tagger.tag(sentence);\n",
    "    System.out.println(Arrays.toString(tags));\n",
    "    System.out.println();\n",
    "    \n",
    "    System.out.println(\"Probabilities of tags: \");\n",
    "    double tagProbabilities[] = tagger.probs();\n",
    "    Arrays.stream(tagProbabilities).forEach(System.out::println);\n",
    "    System.out.println();\n",
    "    \n",
    "    System.out.println(\"Tags as sequences (contains probabilities: \");\n",
    "    Sequence topSequences[] = tagger.topKSequences(sentence);\n",
    "    System.out.println(Arrays.toString(topSequences));\n",
    "}\n",
    "System.out.println(\"[...Finished]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the Chunker model and place it in the ../shared folder (using curl or wget and the system cell magic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if model en-chunker.bin (en) exists...\n",
      "Downloading model en-chunker.bin (en) in '../shared/'\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  3 2500k    3 88884    0     0   133k      0  0:00:18 --:--:--  0:00:18  132k\n",
      " 61 2500k   61 1543k    0     0   759k      0  0:00:03  0:00:02  0:00:01  759k\n",
      " 80 2500k   80 2000k    0     0   629k      0  0:00:03  0:00:03 --:--:--  629k\n",
      " 97 2500k   97 2447k    0     0   650k      0  0:00:03  0:00:03 --:--:--  650k\n",
      "100 2500k  100 2500k    0     0   659k      0  0:00:03  0:00:03 --:--:--  659k\n"
     ]
    }
   ],
   "source": [
    "%system ../opennlp/chunker.sh --downloadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the [en] Chunker model called en-chunker.bin from the ../shared folder and show a simple example of chunking a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Started...]\n",
      "Sentence: [Rockwell, International, Corp., 's, Tulsa, unit, said, it, signed, a, tentative, agreement, extending, its, contract, with, Boeing, Co., to, provide, structural, parts, for, Boeing, 's, 747, jetliners, .]\n",
      "\n",
      "Tags chunked: [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O]\n",
      "\n",
      "Tags chunked (with probabilities): [-0.3533550124421968 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -4.9833651782143225 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -5.207232108117287 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, I-NP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -5.250640871618706 [B-NP, I-NP, I-NP, B-NP, I-NP, O, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -5.2542712803928815 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, B-VP, O], -5.669524713139481 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-NP, B-NP, B-NP, I-NP, I-NP, O], -5.802479037079788 [B-NP, I-NP, B-PP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -5.811282463417802 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, O, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -5.878077943396101 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-LST, B-NP, B-NP, I-NP, I-NP, O], -5.960383921186912 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-ADVP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O]]\n",
      "\n",
      "[...Finished]\n"
     ]
    }
   ],
   "source": [
    "import java.io.InputStream;\n",
    "import java.io.FileInputStream;\n",
    "import opennlp.tools.chunker.ChunkerModel;\n",
    "import opennlp.tools.chunker.ChunkerME;\n",
    "import java.util.Arrays;\n",
    "\n",
    "System.out.println(\"[Started...]\");\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/en-chunker.bin\")){\n",
    "  ChunkerModel model = new ChunkerModel(modelIn);\n",
    "  ChunkerME chunker = new ChunkerME(model);\n",
    "\n",
    "  String sentence[] = new String[] { \"Rockwell\", \"International\", \"Corp.\", \"'s\",\n",
    "    \"Tulsa\", \"unit\", \"said\", \"it\", \"signed\", \"a\", \"tentative\", \"agreement\",\n",
    "    \"extending\", \"its\", \"contract\", \"with\", \"Boeing\", \"Co.\", \"to\",\n",
    "    \"provide\", \"structural\", \"parts\", \"for\", \"Boeing\", \"'s\", \"747\",\n",
    "    \"jetliners\", \".\" };\n",
    "\n",
    "  String pos[] = new String[] { \"NNP\", \"NNP\", \"NNP\", \"POS\", \"NNP\", \"NN\",\n",
    "    \"VBD\", \"PRP\", \"VBD\", \"DT\", \"JJ\", \"NN\", \"VBG\", \"PRP$\", \"NN\", \"IN\",\n",
    "    \"NNP\", \"NNP\", \"TO\", \"VB\", \"JJ\", \"NNS\", \"IN\", \"NNP\", \"POS\", \"CD\", \"NNS\",\n",
    "    \".\" };\n",
    "\n",
    "  String tag[] = chunker.chunk(sentence, pos);\n",
    "  double probs[] = chunker.probs();\n",
    "  Sequence topSequences[] = chunker.topKSequences(sentence, pos);\n",
    "  \n",
    "  System.out.println(\"Sentence: \" + Arrays.toString(sentence) + \"\\n\");\n",
    "  System.out.println(\"Tags chunked: \" + Arrays.toString(tag) + \"\\n\");\n",
    "  System.out.println(\"Tags chunked (with probabilities): \" + Arrays.toString(topSequences) + \"\\n\");\n",
    "}\n",
    "System.out.println(\"[...Finished]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the Parser chunking model and place it in the ../shared folder (using curl or wget and the system cell magic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if model en-parser-chunking.bin (en) exists...\n",
      "Downloading model en-parser-chunking.bin (en) in '../shared/'\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 34.6M    0 42802    0     0  72792      0  0:08:19 --:--:--  0:08:19 72792\n",
      "  3 34.6M    3 1266k    0     0   818k      0  0:00:43  0:00:01  0:00:42  817k\n",
      "  5 34.6M    5 1798k    0     0   704k      0  0:00:50  0:00:02  0:00:48  704k\n",
      "  7 34.6M    7 2567k    0     0   643k      0  0:00:55  0:00:03  0:00:52  643k\n",
      " 10 34.6M   10 3587k    0     0   716k      0  0:00:49  0:00:05  0:00:44  716k\n",
      " 10 34.6M   10 3587k    0     0   596k      0  0:00:59  0:00:06  0:00:53  653k\n",
      " 12 34.6M   12 4568k    0     0   697k      0  0:00:50  0:00:06  0:00:44  660k\n",
      " 12 34.6M   12 4614k    0     0   578k      0  0:01:01  0:00:07  0:00:54  518k\n",
      " 15 34.6M   15 5635k    0     0   611k      0  0:00:58  0:00:09  0:00:49  587k\n",
      " 15 34.6M   15 5647k    0     0   562k      0  0:01:03  0:00:10  0:00:53  409k\n",
      " 18 34.6M   18 6659k    0     0   595k      0  0:00:59  0:00:11  0:00:48  593k\n",
      " 18 34.6M   18 6662k    0     0   556k      0  0:01:03  0:00:11  0:00:52  385k\n",
      " 21 34.6M   21 7683k    0     0   577k      0  0:01:01  0:00:13  0:00:48  575k\n",
      " 21 34.6M   21 7687k    0     0   547k      0  0:01:04  0:00:14  0:00:50  425k\n",
      " 24 34.6M   24 8707k    0     0   570k      0  0:01:02  0:00:15  0:00:47  585k\n",
      " 24 34.6M   24 8710k    0     0   545k      0  0:01:05  0:00:15  0:00:50  428k\n",
      " 27 34.6M   27 9731k    0     0   568k      0  0:01:02  0:00:17  0:00:45  598k\n",
      " 27 34.6M   27 9739k    0     0   540k      0  0:01:05  0:00:18  0:00:47  435k\n",
      " 30 34.6M   30 10.4M    0     0   577k      0  0:01:01  0:00:18  0:00:43  670k\n",
      " 30 34.6M   30 10.5M    0     0   535k      0  0:01:06  0:00:20  0:00:46  426k\n",
      " 33 34.6M   33 11.5M    0     0   558k      0  0:01:03  0:00:21  0:00:42  598k\n",
      " 33 34.6M   33 11.5M    0     0   535k      0  0:01:06  0:00:22  0:00:44  417k\n",
      " 36 34.6M   36 12.5M    0     0   550k      0  0:01:04  0:00:23  0:00:41  586k\n",
      " 36 34.6M   36 12.5M    0     0   531k      0  0:01:06  0:00:24  0:00:42  378k\n",
      " 38 34.6M   38 13.5M    0     0   546k      0  0:01:04  0:00:25  0:00:39  586k\n",
      " 39 34.6M   39 13.5M    0     0   531k      0  0:01:06  0:00:26  0:00:40  419k\n",
      " 41 34.6M   41 14.4M    0     0   558k      0  0:01:03  0:00:26  0:00:37  672k\n",
      " 41 34.6M   41 14.5M    0     0   529k      0  0:01:07  0:00:28  0:00:39  428k\n",
      " 44 34.6M   44 15.5M    0     0   545k      0  0:01:05  0:00:29  0:00:36  610k\n",
      " 44 34.6M   44 15.5M    0     0   529k      0  0:01:07  0:00:29  0:00:38  438k\n",
      " 47 34.6M   47 16.5M    0     0   540k      0  0:01:05  0:00:31  0:00:34  584k\n",
      " 47 34.6M   47 16.5M    0     0   527k      0  0:01:07  0:00:32  0:00:35  375k\n",
      " 50 34.6M   50 17.5M    0     0   539k      0  0:01:05  0:00:33  0:00:32  596k\n",
      " 50 34.6M   50 17.5M    0     0   527k      0  0:01:07  0:00:34  0:00:33  419k\n",
      " 53 34.6M   53 18.4M    0     0   547k      0  0:01:04  0:00:34  0:00:30  664k\n",
      " 53 34.6M   53 18.5M    0     0   526k      0  0:01:07  0:00:36  0:00:31  431k\n",
      " 56 34.6M   56 19.5M    0     0   538k      0  0:01:05  0:00:37  0:00:28  608k\n",
      " 56 34.6M   56 19.5M    0     0   525k      0  0:01:07  0:00:37  0:00:30  427k\n",
      " 59 34.6M   59 20.5M    0     0   538k      0  0:01:05  0:00:39  0:00:26  612k\n",
      " 59 34.6M   59 20.5M    0     0   524k      0  0:01:07  0:00:39  0:00:28  383k\n",
      " 62 34.6M   62 21.5M    0     0   537k      0  0:01:06  0:00:40  0:00:26  615k\n",
      " 62 34.6M   62 21.5M    0     0   524k      0  0:01:07  0:00:41  0:00:26  420k\n",
      " 64 34.6M   64 22.5M    0     0   533k      0  0:01:06  0:00:43  0:00:23  588k\n",
      " 64 34.6M   64 22.5M    0     0   522k      0  0:01:07  0:00:44  0:00:23  404k\n",
      " 67 34.6M   67 23.5M    0     0   532k      0  0:01:06  0:00:45  0:00:21  590k\n",
      " 67 34.6M   67 23.5M    0     0   523k      0  0:01:07  0:00:46  0:00:21  407k\n",
      " 70 34.6M   70 24.5M    0     0   532k      0  0:01:06  0:00:47  0:00:19  595k\n",
      " 70 34.6M   70 24.5M    0     0   522k      0  0:01:07  0:00:48  0:00:19  424k\n",
      " 73 34.6M   73 25.4M    0     0   536k      0  0:01:06  0:00:48  0:00:18  671k\n",
      " 73 34.6M   73 25.5M    0     0   521k      0  0:01:08  0:00:50  0:00:18  419k\n",
      " 76 34.6M   76 26.4M    0     0   535k      0  0:01:06  0:00:50  0:00:16  659k\n",
      " 76 34.6M   76 26.5M    0     0   521k      0  0:01:08  0:00:52  0:00:16  417k\n",
      " 79 34.6M   79 27.5M    0     0   528k      0  0:01:07  0:00:53  0:00:14  585k\n",
      " 79 34.6M   79 27.5M    0     0   520k      0  0:01:08  0:00:54  0:00:14  380k\n",
      " 81 34.6M   81 28.4M    0     0   533k      0  0:01:06  0:00:54  0:00:12  669k\n",
      " 82 34.6M   82 28.5M    0     0   521k      0  0:01:08  0:00:56  0:00:12  389k\n",
      " 85 34.6M   85 29.5M    0     0   529k      0  0:01:07  0:00:57  0:00:10  610k\n",
      " 85 34.6M   85 29.5M    0     0   520k      0  0:01:08  0:00:58  0:00:10  426k\n",
      " 88 34.6M   88 30.5M    0     0   527k      0  0:01:07  0:00:59  0:00:08  598k\n",
      " 88 34.6M   88 30.5M    0     0   520k      0  0:01:08  0:01:00  0:00:08  391k\n",
      " 90 34.6M   90 31.5M    0     0   528k      0  0:01:07  0:01:00  0:00:07  618k\n",
      " 90 34.6M   90 31.5M    0     0   520k      0  0:01:08  0:01:01  0:00:07  417k\n",
      " 93 34.6M   93 32.4M    0     0   531k      0  0:01:06  0:01:02  0:00:04  673k\n",
      " 93 34.6M   93 32.5M    0     0   519k      0  0:01:08  0:01:04  0:00:04  429k\n",
      " 96 34.6M   96 33.5M    0     0   527k      0  0:01:07  0:01:05  0:00:02  607k\n",
      " 96 34.6M   96 33.5M    0     0   519k      0  0:01:08  0:01:06  0:00:02  401k\n",
      " 99 34.6M   99 34.3M    0     0   529k      0  0:01:07  0:01:06  0:00:01  647k\n",
      " 99 34.6M   99 34.5M    0     0   519k      0  0:01:08  0:01:08 --:--:--  386k\n",
      "100 34.6M  100 34.6M    0     0   519k      0  0:01:08  0:01:08 --:--:--  520k\n"
     ]
    }
   ],
   "source": [
    "%system ../opennlp/parser.sh --downloadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the [en] Parser chunker model called en-parser-chunker.bin from the ../shared folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Started...]\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "(TOP (NP (NP (DT The) (JJ quick) (JJ brown) (NN fox) (NNS jumps)) (PP (IN over) (NP (DT the) (JJ lazy) (NN dog.)))))\n",
      "[...Finished]\n"
     ]
    }
   ],
   "source": [
    "import java.io.InputStream;\n",
    "import java.io.FileInputStream;\n",
    "import opennlp.tools.parser.Parse;\n",
    "import opennlp.tools.parser.Parser;\n",
    "import opennlp.tools.parser.ParserModel;\n",
    "import opennlp.tools.parser.ParserFactory;\n",
    "import opennlp.tools.cmdline.parser.ParserTool; \n",
    "import java.util.Arrays;\n",
    "\n",
    "System.out.println(\"[Started...]\");\n",
    "Parse topParses[];\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/en-parser-chunking.bin\")){\n",
    "\n",
    "  ParserModel model = new ParserModel(modelIn);\n",
    "  Parser parser = ParserFactory.create(model);\n",
    "\n",
    "  String sentence = \"The quick brown fox jumps over the lazy dog.\";\n",
    "  topParses = ParserTool.parseLine(sentence, parser, 1);\n",
    "  \n",
    "  System.out.println(\"Sentence: \" + sentence + \"\\n\");\n",
    "  Arrays.stream(topParses).forEach(eachParse -> eachParse.show());\n",
    "}\n",
    "System.out.println(\"[...Finished]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For more resources please refer to [Apache OpenNLP README](https://github.com/neomatrix369/nlp-java-jvm-example/blob/master/images/java/opennlp/README.md) and [Apache OpenNLP Resources](https://github.com/neomatrix369/nlp-java-jvm-example/blob/master/images/java/opennlp/README.md#resources)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.4+11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
