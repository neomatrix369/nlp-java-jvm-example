{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out the version info of the underlying JDK/JVM on which this notebook is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.version: 9.0.4\n"
     ]
    }
   ],
   "source": [
    "System.out.println(\"java.version: \" + System.getProperty(\"java.version\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.specification.version: 9\n",
      "java.runtime.version: 9.0.4+11\n"
     ]
    }
   ],
   "source": [
    "System.out.println(\"java.specification.version: \" + System.getProperty(\"java.specification.version\"));\n",
    "System.out.println(\"java.runtime.version: \" + System.getProperty(\"java.runtime.version\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java runtime VM version: 9.0.4+11\n"
     ]
    }
   ],
   "source": [
    "import java.lang.management.ManagementFactory;\n",
    "\n",
    "System.out.println(\"java runtime VM version: \" + ManagementFactory.getRuntimeMXBean().getVmVersion());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Apache OpenNLP jar files located in the ../shared/apache-opennlp-1.9.1/lib/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "List<String> addedJars = %jars \"../shared/apache-opennlp-1.9.1/lib/*.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[/home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jackson-annotations-2.8.4.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/morfologik-stemming-2.1.3.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/grizzly-http-server-2.3.28.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jersey-common-2.25.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/morfologik-fsa-2.1.3.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jersey-media-jaxb-2.25.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jersey-guava-2.25.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/hk2-api-2.5.0-b30.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/grizzly-http-2.3.28.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jackson-jaxrs-base-2.8.4.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/opennlp-morfologik-addon-1.9.1.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/validation-api-1.1.0.Final.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jersey-server-2.25.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/grizzly-framework-2.3.28.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/javax.annotation-api-1.2.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/opennlp-brat-annotator-1.9.1.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/morfologik-tools-2.1.3.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/osgi-resource-locator-1.0.1.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/morfologik-fsa-builders-2.1.3.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/javassist-3.20.0-GA.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/hk2-utils-2.5.0-b30.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jackson-core-2.8.4.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jackson-databind-2.8.4.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/javax.inject-2.5.0-b30.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/opennlp-tools-1.9.1.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/hppc-0.7.1.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/opennlp-uima-1.9.1.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/hk2-locator-2.5.0-b30.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/javax.ws.rs-api-2.0.1.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jersey-container-grizzly2-http-2.25.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jersey-media-json-jackson-2.25.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jcommander-1.48.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/aopalliance-repackaged-2.5.0-b30.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jersey-entity-filtering-2.25.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jersey-client-2.25.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jackson-jaxrs-json-provider-2.8.4.jar, /home/jovyan/work/./../shared/apache-opennlp-1.9.1/lib/jackson-module-jaxb-annotations-2.8.4.jar]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addedJars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to find out about each of the classes enlisted in this notebook please refer to the Java API JavaDocs at  https://opennlp.apache.org/docs/1.9.1/apidocs/opennlp-tools/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Detector API: \n",
    "##### Import the Language detecting model called langdetect-183.bin from the \"../shared/\" folder, and show a simple example detecting a language of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This is a sample text.\n",
      "Best language: lat\n",
      "Best language confidence: 0.017774467481479657\n",
      "\n",
      "Predict languages (with confidence): [tur (0.009708737864077673), bel (0.009708737864077673), san (0.009708737864077673), ara (0.009708737864077673), mon (0.009708737864077673), tel (0.009708737864077673), sin (0.009708737864077673), pes (0.009708737864077673), min (0.009708737864077673), cmn (0.009708737864077673), aze (0.009708737864077673), fao (0.009708737864077673), ita (0.009708737864077673), ceb (0.009708737864077673), mkd (0.009708737864077673), eng (0.009708737864077673), nno (0.009708737864077673), lvs (0.009708737864077673), kor (0.009708737864077673), som (0.009708737864077673), swa (0.009708737864077673), hun (0.009708737864077673), fra (0.009708737864077673), nld (0.009708737864077673), mlt (0.009708737864077673), bak (0.009708737864077673), ekk (0.009708737864077673), ron (0.009708737864077673), gle (0.009708737864077673), hin (0.009708737864077673), est (0.009708737864077673), tha (0.009708737864077673), slk (0.009708737864077673), ltz (0.009708737864077673), kan (0.009708737864077673), eus (0.009708737864077673), epo (0.009708737864077673), bos (0.009708737864077673), pol (0.009708737864077673), nep (0.009708737864077673), lit (0.009708737864077673), war (0.009708737864077673), srp (0.009708737864077673), ces (0.009708737864077673), che (0.009708737864077673), lav (0.009708737864077673), nds (0.009708737864077673), dan (0.009708737864077673), mar (0.009708737864077673), nan (0.009708737864077673), glg (0.009708737864077673), gsw (0.009708737864077673), fry (0.009708737864077673), uzb (0.009708737864077673), mal (0.009708737864077673), vol (0.009708737864077673), fas (0.009708737864077673), msa (0.009708737864077673), cym (0.009708737864077673), nob (0.009708737864077673), ben (0.009708737864077673), kaz (0.009708737864077673), heb (0.009708737864077673), bre (0.009708737864077673), jav (0.009708737864077673), sqi (0.009708737864077673), kir (0.009708737864077673), cat (0.009708737864077673), oci (0.009708737864077673), vie (0.009708737864077673), kat (0.009708737864077673), tam (0.009708737864077673), tgk (0.009708737864077673), mri (0.009708737864077673), slv (0.009708737864077673), lat (0.009708737864077673), tgl (0.009708737864077673), pan (0.009708737864077673), swe (0.009708737864077673), lim (0.009708737864077673), tat (0.009708737864077673), ell (0.009708737864077673), afr (0.009708737864077673), pus (0.009708737864077673), isl (0.009708737864077673), sun (0.009708737864077673), urd (0.009708737864077673), hye (0.009708737864077673), hrv (0.009708737864077673), ast (0.009708737864077673), rus (0.009708737864077673), spa (0.009708737864077673), ind (0.009708737864077673), pnb (0.009708737864077673), bul (0.009708737864077673), plt (0.009708737864077673), deu (0.009708737864077673), zul (0.009708737864077673), ukr (0.009708737864077673), jpn (0.009708737864077673), por (0.009708737864077673), guj (0.009708737864077673), fin (0.009708737864077673)]\n"
     ]
    }
   ],
   "source": [
    "import opennlp.tools.langdetect.LanguageDetectorModel;\n",
    "import opennlp.tools.langdetect.LanguageDetectorME;\n",
    "import opennlp.tools.langdetect.LanguageDetector;\n",
    "import opennlp.tools.langdetect.Language;\n",
    "\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/langdetect-183.bin\")) {\n",
    "    LanguageDetectorModel langModel = new LanguageDetectorModel(modelIn);\n",
    "    String inputText = \"This is a sample text.\";\n",
    "    System.out.println(\"Sentence: \" + inputText);\n",
    "\n",
    "    // Get the most probable language\n",
    "    LanguageDetector myCategorizer = new LanguageDetectorME(langModel);\n",
    "    Language bestLanguage = myCategorizer.predictLanguage(inputText);\n",
    "    System.out.println(\"Best language: \" + bestLanguage.getLang());\n",
    "    System.out.println(\"Best language confidence: \" + bestLanguage.getConfidence());\n",
    "\n",
    "    // Get an array with the most probable languages\n",
    "    Language[] languages = myCategorizer.predictLanguages(\"\");\n",
    "    System.out.println(\"\");\n",
    "    System.out.println(\"Predict languages (with confidence): \" + Arrays.toString(languages));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apparantly it detects this to be Latin, instead of English \n",
    "maybe the language detecting model needs more training.\n",
    "See https://opennlp.apache.org/docs/1.9.1/manual/opennlp.html#tools.langdetect.training on how this can be achieved**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Detection API\n",
    "##### Import the [en] Sentence detecting model called en-sent.bin from the \"../shared/\" folder, and show a simple example detecting a language of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:   First sentence. Second sentence. \n",
      "[First sentence., Second sentence.]\n",
      "\n",
      "[[2..17), [18..34)]\n"
     ]
    }
   ],
   "source": [
    "import opennlp.tools.sentdetect.SentenceModel;\n",
    "import opennlp.tools.sentdetect.SentenceDetectorME;\n",
    "import opennlp.tools.util.Span;\n",
    "\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/en-sent.bin\")) {\n",
    "  SentenceModel model = new SentenceModel(modelIn);\n",
    "  SentenceDetectorME sentenceDetector = new SentenceDetectorME(model);\n",
    "  String sentence = \"  First sentence. Second sentence. \";\n",
    "  System.out.println(\"Sentence: \" + sentence);\n",
    "  String sentences[] = sentenceDetector.sentDetect(sentence);\n",
    "  System.out.println(Arrays.toString(sentences));\n",
    "  Span sentencesUsingSpan[] = sentenceDetector.sentPosDetect(sentence);\n",
    "  System.out.println();\n",
    "  System.out.println(Arrays.toString(sentencesUsingSpan));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see the two ways to use the SentenceDetect API to detect sentences in a piece of text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer API\n",
    "##### Load the [en] Tokenizer model called en-token.bin from the ../shared folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: An input sample sentence.\n",
      "[An, input, sample, sentence, .]\n",
      "Probabilities of each of the tokens above\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9956236737394807\n",
      "1.0\n",
      "\n",
      "[[0..2), [3..8), [9..15), [16..24), [24..25)]\n"
     ]
    }
   ],
   "source": [
    "import opennlp.tools.tokenize.TokenizerModel;\n",
    "import opennlp.tools.tokenize.TokenizerME;\n",
    "import opennlp.tools.tokenize.Tokenizer;\n",
    "import opennlp.tools.util.Span;\n",
    "import java.util.Arrays;\n",
    "\n",
    "try(InputStream modelIn = new FileInputStream(\"../shared/en-token.bin\")) {\n",
    "    TokenizerModel model = new TokenizerModel(modelIn);\n",
    "    TokenizerME tokenizer = new TokenizerME(model);\n",
    "    String sentence = \"An input sample sentence.\";\n",
    "    System.out.println(\"Sentence: \" + sentence);    \n",
    "    String tokens[] = tokenizer.tokenize(sentence);\n",
    "    System.out.println(Arrays.toString(tokens));\n",
    "    double tokensProbabilies[] = tokenizer.getTokenProbabilities();\n",
    "    System.out.println(\"Probabilities of each of the tokens above\");\n",
    "    Arrays.stream(tokensProbabilies).forEach(System.out::println);\n",
    "    System.out.println();\n",
    "    Span tokensUsingSpans[] = tokenizer.tokenizePos(sentence);\n",
    "    System.out.println(Arrays.toString(tokensUsingSpans));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Finder API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: [Pierre, is, from, Paris, France.]\n",
      "[[0..1) person]\n",
      "Sentence: [John, is, from, London, England.]\n",
      "[[0..1) person]\n"
     ]
    }
   ],
   "source": [
    "import opennlp.tools.namefind.TokenNameFinderModel;\n",
    "import opennlp.tools.namefind.NameFinderME;\n",
    "import opennlp.tools.util.Span;\n",
    "\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/en-ner-person.bin\")) {\n",
    "   TokenNameFinderModel model = new TokenNameFinderModel(modelIn);\n",
    "   NameFinderME nameFinder = new NameFinderME(model);\n",
    "   // The sentence has to be split into words and passed to the Name finder function\n",
    "   String documents[][][] = new String[][][] {{{\"Pierre\",\"is\", \"from\", \"Paris\", \"France.\"}, {\"John\", \"is\", \"from\", \"London\", \"England.\"}}};\n",
    "   for (String document[][]: documents) {\n",
    "      for (String sentence[]: document) {\n",
    "          System.out.println(\"Sentence: \" + Arrays.toString(sentence));\n",
    "          Span nameSpans[] = nameFinder.find(sentence);\n",
    "          System.out.println(Arrays.toString(nameSpans));\n",
    "      }\n",
    "      nameFinder.clearAdaptiveData();\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see above, it has detected the name of the person in both sentences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of speech (POS) Tagger API\n",
    "##### Load the [en] PoS model called en-pos-maxent.bin from the ../shared folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Most, large, cities, in, the, US, had, morning, and, afternoon, newspapers, .]\n",
      "[JJS, JJ, NNS, IN, DT, NNP, VBD, NN, CC, NN, NNS, .]\n",
      "\n",
      "Probabilities of tags\n",
      "0.6005488809717314\n",
      "0.9346347227057236\n",
      "0.9928943439421191\n",
      "0.993711911129381\n",
      "0.9959619800700815\n",
      "0.9632635300742168\n",
      "0.96904256131942\n",
      "0.936549747737236\n",
      "0.9706281118634225\n",
      "0.8831901977922334\n",
      "0.9711019283924753\n",
      "0.9931572030890747\n",
      "\n",
      "Tags as sequences\n",
      "[-0.9196402685290461 [JJS, JJ, NNS, IN, DT, NNP, VBD, NN, CC, NN, NNS, .], -1.4538683571912276 [RBS, JJ, NNS, IN, DT, NNP, VBD, NN, CC, NN, NNS, .], -5.124416242584632 [JJS, JJ, NNS, IN, DT, PRP, VBD, NN, CC, NN, NNS, .]]\n"
     ]
    }
   ],
   "source": [
    "import opennlp.tools.postag.POSModel;\n",
    "import opennlp.tools.postag.POSTaggerME;\n",
    "import opennlp.tools.util.Sequence;\n",
    "import java.util.Arrays;\n",
    "\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/en-pos-maxent.bin\")) {\n",
    "    POSModel model = new POSModel(modelIn);\n",
    "    POSTaggerME tagger = new POSTaggerME(model);\n",
    "\n",
    "    // The sentence has to be split into words and passed to the POS Tagger function\n",
    "    String sentence[] = new String[]{\"Most\", \"large\", \"cities\", \"in\", \"the\", \"US\", \"had\",\n",
    "                             \"morning\", \"and\", \"afternoon\", \"newspapers\", \".\"};\n",
    "    System.out.println(\"Sentence: \" + Arrays.toString(sentence));\n",
    "    String tags[] = tagger.tag(sentence);\n",
    "    System.out.println(Arrays.toString(tags));\n",
    "    System.out.println();\n",
    "    \n",
    "    System.out.println(\"Probabilities of tags: \");\n",
    "    double tagProbabilities[] = tagger.probs();\n",
    "    Arrays.stream(tagProbabilities).forEach(System.out::println);\n",
    "    System.out.println();\n",
    "    \n",
    "    System.out.println(\"Tags as sequences (contains probabilities: \");\n",
    "    Sequence topSequences[] = tagger.topKSequences(sentence);\n",
    "    System.out.println(Arrays.toString(topSequences));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking API\n",
    "#### Load the [en] Chunker model called en-chunker.bin from the ../shared folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: [Rockwell, International, Corp., 's, Tulsa, unit, said, it, signed, a, tentative, agreement, extending, its, contract, with, Boeing, Co., to, provide, structural, parts, for, Boeing, 's, 747, jetliners, .]\n",
      "\n",
      "Tags chunked: [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O]\n",
      "\n",
      "Tags chunked (with probabilities): [-0.3533550124421968 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -4.9833651782143225 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -5.207232108117287 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, I-NP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -5.250640871618706 [B-NP, I-NP, I-NP, B-NP, I-NP, O, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -5.2542712803928815 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, B-VP, O], -5.669524713139481 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-NP, B-NP, B-NP, I-NP, I-NP, O], -5.802479037079788 [B-NP, I-NP, B-PP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -5.811282463417802 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, O, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O], -5.878077943396101 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-VP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-LST, B-NP, B-NP, I-NP, I-NP, O], -5.960383921186912 [B-NP, I-NP, I-NP, B-NP, I-NP, I-NP, B-ADVP, B-NP, B-VP, B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, B-NP, I-NP, I-NP, O]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import opennlp.tools.chunker.ChunkerModel;\n",
    "import opennlp.tools.chunker.ChunkerME;\n",
    "import java.util.Arrays;\n",
    "\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/en-chunker.bin\")){\n",
    "  ChunkerModel model = new ChunkerModel(modelIn);\n",
    "  ChunkerME chunker = new ChunkerME(model);\n",
    "\n",
    "  String sentence[] = new String[] { \"Rockwell\", \"International\", \"Corp.\", \"'s\",\n",
    "    \"Tulsa\", \"unit\", \"said\", \"it\", \"signed\", \"a\", \"tentative\", \"agreement\",\n",
    "    \"extending\", \"its\", \"contract\", \"with\", \"Boeing\", \"Co.\", \"to\",\n",
    "    \"provide\", \"structural\", \"parts\", \"for\", \"Boeing\", \"'s\", \"747\",\n",
    "    \"jetliners\", \".\" };\n",
    "\n",
    "  String pos[] = new String[] { \"NNP\", \"NNP\", \"NNP\", \"POS\", \"NNP\", \"NN\",\n",
    "    \"VBD\", \"PRP\", \"VBD\", \"DT\", \"JJ\", \"NN\", \"VBG\", \"PRP$\", \"NN\", \"IN\",\n",
    "    \"NNP\", \"NNP\", \"TO\", \"VB\", \"JJ\", \"NNS\", \"IN\", \"NNP\", \"POS\", \"CD\", \"NNS\",\n",
    "    \".\" };\n",
    "\n",
    "  String tag[] = chunker.chunk(sentence, pos);\n",
    "  double probs[] = chunker.probs();\n",
    "  Sequence topSequences[] = chunker.topKSequences(sentence, pos);\n",
    "  \n",
    "  System.out.println(\"Sentence: \" + Arrays.toString(sentence) + \"\\n\");\n",
    "  System.out.println(\"Tags chunked: \" + Arrays.toString(tag) + \"\\n\");\n",
    "  System.out.println(\"Tags chunked (with probabilities): \" + Arrays.toString(topSequences) + \"\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing API\n",
    "#### Load the [en] Parsing model called en-parser-chunking.bin from the ../shared folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started...\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "...Finished\n"
     ]
    }
   ],
   "source": [
    "import opennlp.tools.parser.Parse;\n",
    "import opennlp.tools.parser.Parser;\n",
    "import opennlp.tools.parser.ParserModel;\n",
    "import opennlp.tools.parser.ParserFactory;\n",
    "import opennlp.tools.cmdline.parser.ParserTool; \n",
    "import java.util.Arrays;\n",
    "\n",
    "System.out.println(\"Started...\");\n",
    "try (InputStream modelIn = new FileInputStream(\"../shared/en-parser-chunking.bin\")){\n",
    "\n",
    "  ParserModel model = new ParserModel(modelIn);\n",
    "  Parser parser = ParserFactory.create(model);\n",
    "\n",
    "  String sentence = \"The quick brown fox jumps over the lazy dog.\";\n",
    "  Parse topParses[] = ParserTool.parseLine(sentence, parser, 1);\n",
    "  \n",
    "  System.out.println(\"Sentence: \" + sentence + \"\\n\");\n",
    "  Arrays.stream(topParses).forEach(System.out::println);\n",
    "}\n",
    "System.out.println(\"...Finished\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.4+11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
